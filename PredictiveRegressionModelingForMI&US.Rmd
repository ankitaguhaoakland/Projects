---
title: "Comparative Linear Regression Modeling For Ozone Data in MI and in US"
output: html_document
author: "Ankita Guha and Kara Marsh"
date: "April 22, 2018"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading the necessary libraries
```{r libraries}
library(dplyr)
library(ggplot2)
library(coefplot)
library(reshape2)
library(scales)
library(boot)
library(e1071) 
```

**Data Preparation**
```{r}
aqAll <- read.table("data/eda_ozone.csv", header = TRUE, sep = ",")
aqAll$X.1 <- aqAll$X <-NULL
aqMI <- read.table("data/mi_ozone.csv", header = TRUE, sep = ",")
aqMI$X.1 <- aqMI$X <- aqMI$exceptional_data_count <- NULL

```

**Partition Data into Training & Test Data Sets for MI**
```{r}
set.seed(828)
sample_size <- ceiling(0.30 * nrow(aqMI))
testrecs <- sample(nrow(aqMI),sample_size)
MI_test <- aqMI[testrecs,]
MI_train <- aqMI[-testrecs,]  # Negative in front of vector means "not in"
rm(aqMI) # No sense keeping a copy of the entire dataset around
```


**Partition Data into Training & Test Data Sets for US**
```{r}
set.seed(828)
sample_size <- ceiling(0.30 * nrow(aqAll))
testrecs <- sample(nrow(aqAll),sample_size)
All_test <- aqAll[testrecs,]
All_train <- aqAll[-testrecs,]  # Negative in front of vector means "not in"
rm(aqAll) # No sense keeping a copy of the entire dataset around

```


**MI Prediction Models**
Let's do some basic EDA to get some ideas on the variables that might have 
some influence over the Model perfomance.
```{r Histogram of MI Train Data }
ggplot(data = MI_train) + 
  geom_histogram(aes(x = arithmetic_mean), 
                 fill = "steelblue", 
                 colour="Black")
```

```{r Histogram of MI Train Data 95 percentile}
ggplot(data = MI_train) + 
  geom_histogram(aes(x = ninety_five_percentile), 
                 fill = "orange", 
                 colour="Black")
```

```{r}
ggplot(data = MI_train) + 
  geom_histogram(aes(x = arithmetic_standard_dev), 
                 fill = "lightpink", 
                 colour="Black")
```


```{r histo of first_max_value}
ggplot(data = MI_train) + 
  geom_histogram(aes(x = first_max_value), 
                 fill = "violet", 
                 colour="Black")
```


```{r scatterplot of mean & std dev}
ggplot(data = MI_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = arithmetic_standard_dev)) 
```

```{r scatterplot of mean & 1st max value}
ggplot(data = MI_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = first_max_value))
```

```{r scatterplot of mean & 95 percentile}}
ggplot(data = MI_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = ninety_five_percentile))
```



```{r scatterplot of mean & year}}
ggplot(data = MI_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = year))
```


```{r scatterplot of mean over time faceted by county}}
# Scatterplots of arithmetic_mean vs first_max_value faceted along county_name
ggplot(data = MI_train) +
  geom_point(aes(y = arithmetic_mean, 
                 x = first_max_value)) + 
  facet_wrap(~county_name)
```

```{r scatterplot of mean & stdev faceted by year}
ggplot(data = MI_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = arithmetic_standard_dev)) + 
  facet_wrap(~year)
```

```{r scatterplot of mean & stdev faceted by metric used}
ggplot(data = MI_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = arithmetic_standard_dev, 
                 col = metric_used)) + 
  facet_wrap(~metric_used)
```

```{r scatterplot of mean & stdev faceted by method name}
ggplot(data = MI_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = arithmetic_standard_dev, 
                 col = method_name)) + 
  facet_wrap(~method_name)
```

As we can see from our plots that the "Missing Data From Kaggle" are the main 
Test or method_name used to capture the paramter_name or the Air Pollutant 
constituent. 

****Linear Regression Model for Ozone Data of MI****
Let's try to build Linear Regression Model to predict the arithmetic_mean

**Null Model for Comparison with MI Data**
```{r}
meanMI_lm0 <- lm(arithmetic_mean ~ 1, data = MI_train)
summary(meanMI_lm0)
# Compute the MAD value
median(abs(meanMI_lm0$residuals))
```
So we got a pretty low MAD value for the Null Model which is like kind of 
perfect!

Let's visualize the Residuals.
```{r}
ggplot(meanMI_lm0, aes(x=.resid)) + 
  geom_histogram() 
```

Prediction at 95% confidence interval
```{r}
meanMIPredict0 <- predict(meanMI_lm0, 
                          newdata=MI_test, 
                          se.fit=TRUE,
                          interval="prediction", 
                          level=0.95)
```

**MAD Value for Prediction**
```{r}
df0 <- abs(meanMIPredict0$fit[,1]-MI_test["arithmetic_mean"])

(testMad0 <- median(as.numeric(df0$arithmetic_mean)))
```

**Findings on NUll Model for MI Data**
'So, our MAD value for Test Data: 0.003643001 
'        MAD value for Train Data: 0.003508
'Although MAD for Test Data is little more than that of the Train Data, but 
we can almost say that we are 95% confident that our Model0 or Null Model 
has an almost perfect fit.       

```{r}
head(MI_train)
str(MI_train)
```

```{r}
cor(MI_train$arithmetic_mean, MI_train$arithmetic_standard_dev)
```

```{r}
cor(MI_train$arithmetic_mean, MI_train$first_max_value)
cor(MI_train$arithmetic_mean, MI_train$second_max_value)
cor(MI_train$arithmetic_mean, MI_train$third_max_value)
cor(MI_train$arithmetic_mean, MI_train$fourth_max_value)
cor(MI_train$arithmetic_mean, MI_train$ninety_five_percentile)
```
So looks like the arithmetic_mean has a high correlation with the 
ninety_five_percentile value.

**Correlation Matrix**
```{r}
correlationbuild1 <- MI_train %>%
  select(county_name, 
         first_max_value, 
         arithmetic_mean, 
         arithmetic_standard_dev, 
         ninety_five_percentile)

head(correlationbuild1)
```


```{r}
cor(correlationbuild1[, c(2, 3:5)])
```

```{r}
# Putting the Correlation Matrix in a Dataframe
MItraincor <- cor(correlationbuild1[, c(2, 3:5)])

# Melting the Data for ease of plotting
correlationbuild1Melt <- melt(MItraincor, 
                              varnames=c("x", "y"), 
                              value.name = "Correlation")

# Ordering the Correlation According to their Value
correlationbuild1Melt <- correlationbuild1Melt[order(correlationbuild1Melt$Correlation), ]

# Display the Melted Data
correlationbuild1Melt
```


```{r}
# Heatmap of Correlation For Numerical Predictors
ggplot(correlationbuild1Melt, aes(x=x, y=y)) + 
  geom_tile(aes(fill=Correlation)) + 
  scale_fill_gradient2(low = muted("yellow"), 
                       mid = "white", 
                       high = "plum", 
                       guide = guide_colorbar(ticks = FALSE, 
                                              barheight = 10), 
                       limits=c(-1, 1)) + 
  theme_minimal() + 
  labs(x=NULL, y = NULL)
```

**Model 1**
```{r}
# Model 1: Build to Understand the effect of the Predictors on the Response Variable 
mean1 <- lm(arithmetic_mean ~ method_name + year + observation_count + observation_percent + valid_day_count + required_day_count + arithmetic_standard_dev + first_max_value + second_max_value + third_max_value + fourth_max_value + ninety_nine_percentile + ninety_eight_percentile + ninety_five_percentile + ninety_percentile + seventy_five_percentile + fifty_percentile + ten_percentile + date_of_last_change + norm_mean, data = MI_train)

summary(mean1)
```

Okay, so looks like our data is really Normalized as we have a R-Square of 
exactly 1 with a significant low p value. 
```{r}
# Compute the MAD value
median(abs(mean1$residuals))
```

```{r}
mean1$coefficients
```

Let's try to visualize the Model 
```{r}
ggplot(mean1, aes(x=.resid)) + geom_histogram() 
```
From the Residual Analysis, we considered to check our homeskedasticity of 
Variance along two of the most significant Predictors that is the method_name 
and norm_mean. Ideally we would like to see random scatter in our Residual 
Analysis, but for both the plot we can see that the Residuals are not 
scatttered randomly but accumulated over a certain point, forming a Hat 
like pattern indicating that the variance in the residuals is not constant.
```{r}
# Residual Analysis by norm_mean
ggplot(data=mean1, aes(x=.fitted, y = .resid)) + 
  geom_point(aes(color=norm_mean)) + 
  labs(x="Fitted Values", y="Residuals")

# Residual Analysis by method_name
ggplot(data=mean1, 
       aes(x=.fitted, y = .resid)) + 
  geom_point(aes(color=method_name)) + 
  labs(x="Fitted Values", y="Residuals")
```

The Residuals also seemed to have the major influence from the unknown Test 
method_name which was initially blank and we renamed it as "Missing Data 
From Kaggle"
```{r}
# Making Prediction with Test Data at 95% Confidence Interval
meanMIPredict1 <- predict(mean1, newdata=MI_test, se.fit=TRUE,
                         interval="prediction", level=0.95)
```

**MAD Value for Prediction**
```{r}
df1 <- abs(meanMIPredict1$fit[,1]-MI_test["arithmetic_mean"])

(testMad1 <- median(as.numeric(df1$arithmetic_mean)))
```
Looking at our p, value from the Anova Test, looks like we have a very good 
Fit Model!

```{r}
anova(mean1)
```

Our data looks almost like normally distributed. The normal distribution has
skew and kurtosis values of zero. It is a measure of data symmetry. As a rule,
negative skewness indicates that the mean of the data values is less than the 
median, and the data distribution is left-skewed. Kurtosis describes the tail 
shape of the data distribution. The Positive value as seen from our Kurtosis 
indicates that our data has a fat tailed data distribution.

```{r}

skewness(mean1$residuals)
kurtosis(mean1$residuals)
```

Let's clean up some of our memory space for next iteration of analysis.

```{r}
rm(MI_test, MI_train, df1, MItraincor, 
   correlationbuild1, correlationbuild1Melt, 
   mean1, meanMI_lm0, meanMIPredict1)
```

**Fidings on MI Data Prediction**

+ Our Model is significant with an accurate R Squared Value of 1! (Rare Case No Doubt!). 

+ The Null Model which we considered as the base for comparison has a very high 
performance value. So, we had to be cautious while building other Models. But we 
still went ahead to play with some of the variables until we get a Model which we 
thought would make a good fit to be here. After diligently considering some of the 
Model performance we considered this Model as our best Model as compared to our 
Null Model. 

+ MAD Value for Train: 3.231402e-17
  MAD Value for Test: 3.469447e-17, representing signs of very little Overfit! 
  Otherwise perfectly Fit Model even as seen from the Anova Test.

+ But overall Model performance was good mainly due to the data that we choose to 
build and Predict the Model.  

+ It definitely has a lot of scope to do a lot more study with this dataframe. But 
due to our limited Project Scope, we would have to restrict ourselves here. 



**US Prediction**
```{r}
ggplot(data = All_train) + 
  geom_histogram(aes(x = arithmetic_mean), 
                 fill = "steelblue", 
                 colour="Black")
```

```{r}
ggplot(data = All_train) + 
  geom_histogram(aes(x = ninety_five_percentile), 
                 fill = "orange", colour="Black")

ggplot(data = All_train) + 
  geom_histogram(aes(x = arithmetic_standard_dev), 
                 fill = "lightpink", colour="Black")

ggplot(data = All_train) + 
  geom_histogram(aes(x = first_max_value), 
                 fill = "violet", 
                 colour="Black")

ggplot(data = All_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = arithmetic_standard_dev)) 

ggplot(data = All_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = first_max_value))

ggplot(data = All_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = ninety_five_percentile))

ggplot(data = All_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = year))

ggplot(data = All_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = arithmetic_standard_dev, 
                 col = metric_used)) + 
  facet_wrap(~metric_used)

ggplot(data = All_train) + 
  geom_point(aes(y = arithmetic_mean, 
                 x = arithmetic_standard_dev, 
                 col = method_name)) + 
  facet_wrap(~method_name)
```

***Linear Regression Model for Ozone Data of US***
Let's try to build Linear Regression Model to predict the arithmetic_mean

**Null Model for Comparison with US Data**
```{r}
meanAll_lm0 <- lm(arithmetic_mean ~ 1, data = All_train)

summary(meanAll_lm0)

# Compute the MAD value
median(abs(meanAll_lm0$residuals))
```

So as it can be seen that our MAD value for the Null Model for th entire 
US Data on the Pollutant Ozone is pretty much low, which is kind of a perfect
Model for being a Null Model. 
```{r}
ggplot(meanAll_lm0, aes(x=.resid)) + geom_histogram() 
```
Prediction at 95% confidence interval
```{r}
meanAllPredict0 <- predict(meanAll_lm0, newdata=All_test, se.fit=TRUE,
                         interval="prediction", level=0.95)
```

**MAD Value for Prediction**
```{r}
df0All <- abs(meanAllPredict0$fit[,1]-All_test["arithmetic_mean"])

(testMad0 <- median(as.numeric(df0All$arithmetic_mean)))
```

**Findings on US Data Predictions**
So, our MAD value for Test Data: **0.005169092**
        MAD value for Train Data: **0.005163092**
Wow! So we got an exact Test and Train Data Prediction. So we are 95% Confident 
that our Model Prediction for our Model0 for predicting the US Data on the arithmetic
mean of the presence of the Pollutant Ozone is a perfect fit. 

```{r}
cor(All_train$arithmetic_mean, All_train$arithmetic_standard_dev)
```

```{r}
cor(All_train$arithmetic_mean, All_train$first_max_value)
cor(All_train$arithmetic_mean, All_train$second_max_value)
cor(All_train$arithmetic_mean, All_train$third_max_value)
cor(All_train$arithmetic_mean, All_train$fourth_max_value)
cor(All_train$arithmetic_mean, All_train$ninety_five_percentile)
```
The maximum correlation of the arith_mean seems to be with the ninety_five_
percentile for the entire US data based on Ozone.

**Correlation Matrix**
```{r}
correlationbuild2 <- All_train %>%
  select(county_name, 
         first_max_value, 
         arithmetic_mean, 
         arithmetic_standard_dev, 
         ninety_five_percentile)

head(correlationbuild2)
```

```{r}
cor(correlationbuild2[, c(2, 3:5)])
```

```{r}
# Putting the Correlation Matrix in a Dataframe
Alltraincor <- cor(correlationbuild2[, c(2, 3:5)])

# Melting the Data for ease of plotting
correlationbuild2Melt <- melt(Alltraincor, varnames=c("x", "y"), 
                              value.name = "Correlation")

# Ordering the Correlation According to their Value
correlationbuild2Melt <- correlationbuild2Melt[order(correlationbuild2Melt$Correlation), ]

# Display the Melted Data
correlationbuild2Melt
```

```{r}
# Heatmap of Correlation For Numerical Predictors
ggplot(correlationbuild2Melt, aes(x=x, y=y)) + 
  geom_tile(aes(fill=Correlation)) + 
  scale_fill_gradient2(low = muted("red"), 
                       mid = "white", 
                       high = "plum", 
                       guide = guide_colorbar(ticks = FALSE, 
                                              barheight = 10), 
                       limits=c(-1, 1)) + 
  theme_minimal() + 
  labs(x=NULL, y = NULL)
```
**Model 1 for US**
```{r}
# Model 1: Build to Understand the effect of the Predictors on the Response Variable 
mean1US <- lm(arithmetic_mean ~  year + observation_count + observation_percent + valid_day_count + required_day_count + arithmetic_standard_dev + first_max_value + second_max_value + third_max_value + fourth_max_value + ninety_nine_percentile + ninety_eight_percentile + ninety_five_percentile + ninety_percentile + seventy_five_percentile + fifty_percentile + ten_percentile + norm_mean + state_name, data = All_train)

summary(mean1US)
```


```{r}
# Compute the MAD value
median(abs(mean1US$residuals))
```

```{r}
mean1US$coefficients
```

```{r}
ggplot(mean1US, aes(x=.resid)) + 
  geom_histogram() + 
  scale_x_log10()
```

```{r}
# Residual Analysis by state_name
ggplot(data=mean1US, aes(x=.fitted, y = .resid)) + 
  geom_point(aes(color=state_name)) + 
  labs(x="Fitted Values", y="Residuals")

# Residual Analysis by year
ggplot(data=mean1US, aes(x=.fitted, y = .resid)) + 
  geom_point(aes(color=year)) + 
  labs(x="Fitted Values", y="Residuals")
```
Seems like a lot new parameter_name which could have contributed as Pollutant
has been observed as data was gathered over the period of time.

```{r}
# Making Prediction with Test Data and 95% Confidence Interval
meanPredict1 <- predict(mean1US, newdata=All_test, se.fit=TRUE,
                         interval="prediction", level=0.95)
```

**MAD Value for Prediction**
```{r}
df1 <- abs(meanPredict1$fit[,1]-All_test["arithmetic_mean"])

(testMad1 <- median(as.numeric(df1$arithmetic_mean)))
```
Looking at our p, value from the Anova Test, looks like we have a very 
good Fit Model!
```{r}
anova(mean1US)
```

Our data looks almost like normally distributed. The normal distribution has
skew and kurtosis values of zero. It is a measure of data symmetry. As a rule,
Positive skewness would indicate that the mean of the data values is larger 
than the median, and our data distribution is right-skewed. Kurtosis describes 
the tail shape of the data distribution. The Positive value as seen from our 
Kurtosis indicates that our data has a fat tailed data distribution.

```{r}
skewness(mean1US$residuals)
kurtosis(mean1US$residuals)
```


Let's clear our dataframe now

```{r}
rm(All_test, All_train, correlationbuild2, correlationbuild2Melt, 
   df1, mean1US, meanAll_lm0, meanPredict1)
```


**Final Conclusion**

+ The MAD value for the Test data for Model 1 on US data seems to be at a 
little higher side than the Train Data for the entire US data with Ozone 
as the main contributing pollutant. 

+ Our Model is again significant with an accurate R Squared Value of 1! So for 
obvious reason it is not difficult to get that ideal R Square Value, provided 
you take real good time and effort to clean and normalize the data, again that 
also depends on the kind of dataets that you are using for Modeling. 

+ The Null Model which we considered as the base for comparison has a very high
performance value. So, we had to be cautious while building other Models based 
on both MI and US data. But we still went ahead to play with some of the variables
until we get a Model which we thought would make a good fit to be here. After 
diligently considering some of the Model performances we considered these 2 Models
as our best Model as compared to our Null Model for comparing the Ozone data for 
both MI and US. 

+ For *US*, MAD value for Train Data: **1.134721e-09**
        MAD value for Test Data: **1.15255e-09**
Although MAD for Test Data is little more than that of the Train Data, 
indicating a little underfit but we can certainly say that we are 95% 
confident that our Model has an almost perfect fit.
        
+ For *MI*, our MAD value for Train Data: **0.003508** 
        MAD value for Test Data: **0.003643001**
Wow! So we got almost an exact Test and Train Data Prediction. So we are
95% Confident that our Model Prediction for our Model for predicting the 
US Data on the arithmetic mean of the presence of the Pollutant Ozone is 
a perfect fit. 

+ It is important to note that although we appear to be very successful 
with this analysis, we have actually made a very grievous mistake! We have
incorporated variables into our regression that are directly linked to what 
we are trying to predict. In this way, we have an unfair model. This was 
definitely a learning lesson for us!