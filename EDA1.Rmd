---
title: "Exploratory Data Analysis Part 1"
output: html_document
Authors: "Ankita Guha, Kara Marsh, Jacob Martin"
Date: "April 22, 2018"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r reading cleaned dataframe}
eda <- read.table("cleaned2_AQ.csv", header = TRUE, sep = ",")
```

***Proposition 1***
We would like to figure out pollutants distribution across various regions of US based on years. 

**Data Preparation**
To meet the purpose of Proposition 1, let's try to scoop out some of the chunk of datas that we might need to focus on. And then let's try to free some of the memory for the data frame which we aren't using. 

```{r reading into a dataframe for EDA}
library(dplyr)

eda1 <- subset(eda, select = c(latitude, longitude, parameter_name, method_name, year, arithmetic_mean, arithmetic_standard_dev, state_name))
rm(eda) 
```


```{r checking values superficially}
#filter(eda1, !is.na(method_name) | method_name != "")
```

Okay, so let's check out the datas across all the 30 years seperately. For the purpose of the Project's scope limitation we would like to see the data for the year of 1987 and 2017, which indicates the beggining and the end of the air pollutants data available respectively. 

```{r data for all the 30 years}
filter(eda1,year == "1987")
# filter(eda1,year == "1988")
# filter(eda1,year == "1989")
# filter(eda1,year == "1990")
# filter(eda1,year == "1991")
# filter(eda1,year == "1992")
# filter(eda1,year == "1993")
# filter(eda1,year == "1994")
# filter(eda1,year == "1995")
# filter(eda1,year == "1996")
# filter(eda1,year == "1997")
# filter(eda1,year == "1998")
# filter(eda1,year == "1999")
# filter(eda1,year == "2000")
# filter(eda1,year == "2001")
# filter(eda1,year == "2002")
# filter(eda1,year == "2003")
# filter(eda1,year == "2004")
# filter(eda1,year == "2005")
# filter(eda1,year == "2006")
# filter(eda1,year == "2007")
# filter(eda1,year == "2008")
# filter(eda1,year == "2009")
# filter(eda1,year == "2010")
# filter(eda1,year == "2011")
# filter(eda1,year == "2012")
# filter(eda1,year == "2013")
# filter(eda1,year == "2014")
# filter(eda1,year == "2015")
# filter(eda1,year == "2016")
filter(eda1,year == "2017")
```

```{r mean of pollutants for 1987 & 2017}
# Filtering highest and lowest mean of pollutants data for the year 1987 
eda1987 <- eda1 %>% filter(year == "1987") %>% arrange(desc(arithmetic_mean))
eda1 %>% filter(year == "1987") %>% arrange(desc(arithmetic_mean))
eda1 %>% filter(year == "1987") %>% arrange(arithmetic_mean)

# Filtering highest and lowest mean of pollutants data for the year 2017
eda2017 <- eda1 %>% filter(year == "2017") %>% arrange(desc(arithmetic_mean))
eda1 %>% filter(year == "2017") %>% arrange(desc(arithmetic_mean))
eda1 %>% filter(year == "2017") %>% arrange(arithmetic_mean)

# Creating Another Data Frame for comparing Mean Value of Pollutants Data Across the Year 1987 & 2017
eda2 <- eda1 %>% filter(year == "1987" | year == "2017") %>% arrange(desc(arithmetic_mean))
```


```{r identify total pollutants and test method used}
library(plyr)
sapply(eda1987, count)
```

So apparently looks like there are 143 pollutants listed and the number of tests done in that year of 1987 is 114. We need to next find out which are the top tests and the pollutants name which are mostly found in the year.  

## Histogram of Arithmetic Mean of Pollutants Distribution Across Entire US for 30 Years
```{r histogram}
library(ggplot2)
ggplot(data = eda1) + geom_histogram(aes(x = arithmetic_mean), fill = "plum", color = "black") + labs(
    x = "Pollutant Mean",                                          # x axis title
    y = "Actual Pollutants",                                       # y axis title
    title = "Distribution of pollutants across US for 30 years"    # main title of figure
  ) + scale_x_log10()
```

## Histogram of Arithmetic Mean of Pollutants Distribution Across Entire US for the Year of 1987
```{r}
ggplot(data = eda1987) + geom_histogram(aes(x = arithmetic_mean), fill = "steelblue", color = "black") + labs(
    x = "Pollutant Mean",                                                 # x axis title
    y = "Actual Pollutants",                                       # y axis title
    title = "Distribution of pollutants across US for the year of 1987"       # main title of figure
  ) + scale_x_log10()
```

## Histogram of Arithmetic Mean of Pollutants Distribution Across Entire US for the Year of 2017
```{r}
ggplot(data = eda2017) + geom_histogram(aes(x = arithmetic_mean), fill = "salmon", color = "black") + labs(
    x = "Pollutant Mean",                                                 # x axis title
    y = "Actual Pollutants",                                       # y axis title
    title = "Distribution of pollutants across US for the year of 2017"       # main title of figure
  ) + scale_x_log10()
```

So apparently from the histogram distribution it looks like the distribution of the Pollutants comparatively decreased over the 30 years span. Let's check some other paramters to validate this point. 

Let's check the variation of the mean value of the pollutants across the data frames.

## Density Plots
```{r density plot}
ggplot(eda1, aes(x=arithmetic_mean)) + geom_density(fill = "plum", alpha = 0.90) + scale_x_log10()
ggplot(eda1987, aes(x=arithmetic_mean)) + geom_density(fill = "steelblue", alpha = 0.90) + scale_x_log10()
ggplot(eda2017, aes(x=arithmetic_mean)) + geom_density(fill = "salmon", alpha = 0.90) + scale_x_log10()
```

Looks like the distribution of the pollutants got an almost similar patterns in the entire datasets to that of the year 2017. Howver for the year of 1987 the density of distribution of the pollutants looks small. The only difference in the pollution density distribution is that in the year of 1987 and 2017 where at one point some of the pollutants density seemed to be at higher than that of the 2017.

## Kernel Density Overlaid on Histogram
```{r kernel density overlaid on histogram for 30 years}
# Spanned Over 30 Years
ggplot(eda1, aes(x=arithmetic_mean, y=..density..)) + geom_histogram(fill="plum", color="grey60", size=.2) + geom_density() + scale_x_log10()
```

```{r kernel density overlaid on histogram for year 1987}
# For Year of 1987
ggplot(eda1, aes(x=arithmetic_mean, y=..density..)) + geom_histogram(fill="steelblue", color="grey60", size=.2) + geom_density() + scale_x_log10()
```

```{r kernel density overlaid on histogram for year 2017}
# For Year of 2017
ggplot(eda1, aes(x=arithmetic_mean, y=..density..)) + geom_histogram(fill="salmon", color="grey60", size=.2) + geom_density() + scale_x_log10()
```

## Violin Plot
```{r violin plot}
ggplot(eda1, aes(x="Pollutants", y=arithmetic_mean)) + geom_violin(fill = "plum", alpha = 0.8) + scale_y_log10()
ggplot(eda1987, aes(x="Pollutants", y=arithmetic_mean)) + geom_violin(fill = "steelblue", alpha = 0.8) + scale_y_log10()
ggplot(eda2017, aes(x="Pollutants", y=arithmetic_mean)) + geom_violin(fill = "salmon", alpha = 0.8) + scale_y_log10()
```

## Boxplots
```{r boxplots}
ggplot(eda1, aes(y=arithmetic_mean, x=1)) + geom_boxplot(fill = "plum", alpha = 0.2) + scale_y_log10()
ggplot(eda1987, aes(y=arithmetic_mean, x=1)) + geom_boxplot(fill = "steelblue", alpha = 0.2) + scale_y_log10()
ggplot(eda2017, aes(y=arithmetic_mean, x=1)) + geom_boxplot(fill = "salmon", alpha = 0.2) + scale_y_log10()
```

As seen from both the Boxplot it can be said that:
*For Entire Dataset*: The Median of the entire Pollutant mean is located almost exactly in the middle of the Box indicating a uniform distribution of Pollutants across Entire US as collected over the span of 30 years.

*For 1987 US Data*: The Median of the Pollutants Mean seems to be little more closer to the Upper end of the Spectrum (i.e towards the top whisker) indicating a higher amount of Pollutants distribution across the entire US during the year of 1987. 

*For 2017 US Data*: The Median of the Pollutants Mean seems to be way more closer to the Upper end of the Spectrum (i.e towards the top whisker) indicating a higher amount of Pollutants distribution across the entire US during the year of 2017. However the entire Box size is smaller as compared to that of both the entire dataset as well as for the dataset of 1987, which could be probbaly due to less captured data value in the year of 2017 as compared to 1987. 


Let's try to get a visual comparison of the pollutants data for both the year of 1987 and 2017

```{r boxplots comparing 1987 and 2017}
ggplot(eda2, aes(x=factor(year), y=arithmetic_mean, color=year)) + geom_boxplot() + labs(
    x = "Year",                                                   # x axis title
    y = "Pollutant Mean",                                         # y axis title
    title = "Pollutant Distribution Across Year 1987 & 2017"      # main title of figure 
) + scale_y_log10()
```

And we can say conclusively that Median of the Pollutant Mean is more closer to the left whisker or the Top Spectrum of the Box for both the year of 1987 and 2017. This indicates that the Median of the Pollutant Mean are at a higher side in the year of 2017 as compared to the year of 1987. However the entire difference in the size of both the Boxplots as seen in both the year of 1987 and 2017 seems to be different due to the distribution variation in the Mean of the Pollutants data so collected. 


```{r}
library(ggmap)
library(mapdata)
library(maps)
library(stringr)
library(viridis)
library(maptools)
library(gpclib) 
library(sp)
gpclibPermit()
```

Let's try to check which regions in US we have these pollutants data for.  

## Google Satellite Map
```{r map for entire data in US}
# loading the required packages
# library(ggplot2)
# library(ggmap)

map1 <- get_map(location = c(lon = mean(eda1$longitude), lat = mean(eda1$latitude)), zoom = 4,
                      maptype = "satellite", scale = 2)

#knitr::include_graphics('Documents/Projects/AirQuality/staticmap.png')

ggmap(map1) +
  geom_point(data = eda1, aes(x = longitude, y = latitude, fill = "red", alpha = 0.8), size = 5, shape = 21) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE)
```


Interestingly for the entire data sets looks like the pollutants data are collected from all over the US. 

Let's try to check the regions covered in the year of 1987.

```{r pollutants data covered from regions of US in 1987}
map1987 <- get_map(location = c(lon = mean(eda1987$longitude), lat = mean(eda1987$latitude)), zoom = 4,
                      maptype = "satellite", scale = 2)

ggmap(map1987) +
  geom_point(data = eda1987, aes(x = longitude, y = latitude, fill = "red", alpha = 0.8), size = 5, shape = 21) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE)
```

Let's try to see the same for the year of 2017.

```{r pollutants data covered from Regions of US 2017}
map2017 <- get_map(location = c(lon = mean(eda2017$longitude), lat = mean(eda2017$latitude)), zoom = 4,
                      maptype = "satellite", scale = 2)

ggmap(map2017) +
  geom_point(data = eda2017, aes(x = longitude, y = latitude, fill = "red", alpha = 0.8), size = 5, shape = 21) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE)
```

Let's try to visualize the States with the highest mean pollutants distribution. 
```{r States level pollution distribution}
ggplot(data = eda1) + geom_histogram(aes(x = arithmetic_mean, binwidth = 30000, fill = state_name)) + scale_x_log10() 
```

Okay, from the Histogram distribution, it looks like, some of the States such as Alabama, Alaska, Arizona have a higher level concentration of pollutants that are captured across the datas in these span of 30 years. 


Let's reuse some of the dataframes for our next level of analysis
```{r}
write.csv(eda1, file = "eda1.csv")
write.csv(eda1987, file = "eda1987.csv")
write.csv(eda2017, file = "eda2017.csv") 
write.csv(eda2, file = "eda2.csv")        #Entire US with 2017 & 1987
write.csv(eda2_States, file = "eda2_States.csv")  # 4 States with 2017 & 1987
```

Let's try to free some of the used spaces for better use with our memory and for being able to fit other EDA's conviniently.
```{r clear memory space}
rm(map1, map1987, map2017, eda2_States, eda1, eda1987, eda2, eda2017)
```

**Conclusion**

+ It is interesting to note that the Pollutants data so collected are not in the same units of measurement, so Normalization would have produced much optimized exploratory data analysis. But that would require much more in-depth subject matter expertise & time so that could form a future study itself. 

+ In this study we viewed the pollutants distribution value across US based on the year of 1987 & 2017 as well as for the Pollutants distribution across the Entire US for a span of 30 years. 

+ Also for ease of analysis we have only included the Arithmetic Mean of the Pollutants and have assumed that those are normalized across all the Pollutants so captured in our analysis. Because for obvious reasons different Pollutants or Air Constituent would have different threshold level to be considered as Pollutant. 

+ For the Map to be more explicit in nature, one can increase or decrease the **zoom** argument to acheieve the desired level of Map visual. We have decided our zoom to be at a desired level, for the purpose of providing a suitable aerial view of all the 4 States that we are looking into. 

+ Another interesting fact point that we learned while using ggmap() is that after a certain point of time, query used to fetch map data might not run, if a certain quota of fetching map API data from google is met. We came across an error something like: **geocode failed with status OVER_QUERY_LIMIT, location = "michigan"**, that means that we have run our code many times and hence the IP address has met it's limit to use and fetch API data from Google. 
**Source: https://stackoverflow.com/questions/tagged/google-geocoding-api?page=4&sort=unanswered**




